{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "from typing import List, Optional, Dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_root = '/opt/workspace/data/smc/'\n",
    "if os.path.exists('/Users/dev/'):\n",
    "    path_root = '/Users/dev/work/data/smc/'\n",
    "    \n",
    "print(path_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pathes = glob.glob(path_root+\"b_*\")\n",
    "for n, p in enumerate(train_pathes):\n",
    "    print('{}: {}'.format(n, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pathes = glob.glob(path_root+\"z_*\")\n",
    "for n, p in enumerate(test_pathes):\n",
    "    print('{}: {}'.format(n, p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OpenTextProcessor:\n",
    "    def __init__(self,\n",
    "            min_occurrence: Dict[int,int],\n",
    "            left_cnt: int, right_cnt: int,\n",
    "            min_error_len: int, max_error_len: int,\n",
    "            sw='internal'\n",
    "        ):\n",
    "        self.ks = list(min_occurrence.keys())\n",
    "        self.min_occurrence = min_occurrence\n",
    "        self.left_cnt = left_cnt\n",
    "        self.right_cnt = right_cnt\n",
    "        self.min_error_len = min_error_len\n",
    "        self.max_error_len = max_error_len\n",
    "        \n",
    "        self.alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "        self.alen = len(self.alphabet)\n",
    "        \n",
    "        self.sw = {}\n",
    "        if isinstance(sw, dict):\n",
    "            self.sw = sw\n",
    "        elif sw=='internal':\n",
    "            self.sw = {\n",
    "                'the', 'what', 'is', 'a', 'in', 'i', 'to', 'how', 'of', 'do', 'are', 'and', 'for', 'can', 'you', 't',\n",
    "                'why', 'it', 'my', 'does', 'on', 'or', 'which', 's', 'with', 'if', 'have', 'be', 'an', 'that',\n",
    "                'some', 'get', 'should', 'from', 'your', 'at', 'when', 'like', 'who', 'there', 'will', 'as',\n",
    "                'would', 'not', 'one', 'about', 'where', 'any', 'by', 'me', 'did', 'was', 'we', 'after',\n",
    "                'so', 'they', 'this', 'am', 'has', 'their', 'many', 'than', 'more', 'other', 'but', 'out',\n",
    "                'into', 'm'\n",
    "            }\n",
    "        print('len(sw)={}'.format(len(sw)))\n",
    "        \n",
    "        self.train_texts = None\n",
    "        \n",
    "        print('self.alphabet={}'.format(self.alphabet))\n",
    "        print('len(self.alphabet)={}'.format(len(self.alphabet)))\n",
    "        print('len(self.sw)={}'.format(len(self.sw)))\n",
    "        \n",
    "    def _get_texts_dicts(self, pathes: List[str]):\n",
    "        texts = []\n",
    "        for n, p in enumerate(pathes):\n",
    "            text_p = self._read_txt(p)\n",
    "            if text_p is None:\n",
    "                continue\n",
    "            \n",
    "            text_p = self._preproc_txt(text_p)\n",
    "            print('{}: len={}'.format(p, len(text_p)))\n",
    "            \n",
    "            texts.append({'num':n, 'path':p, 'text':text_p})\n",
    "        return texts\n",
    "        \n",
    "    def fit(self, train_pathes: List[str]):\n",
    "        self.train_texts = self._get_texts_dicts(train_pathes)    \n",
    "        print('len(self.train_texts)={}\\n'.format(len(self.train_texts)))\n",
    "            \n",
    "        self.kgramm_enc = self._get_kgramm_encoder()\n",
    "        print('len(self.kgramm_enc)={}\\n'.format(len(self.kgramm_enc)))\n",
    "        \n",
    "        dfs = {}\n",
    "        for k in self.ks:\n",
    "            print('k={}'.format(k))\n",
    "            dfs[k] = self.create_kgramm_df(k=k, txts=self.train_texts)\n",
    "            print('\\n')\n",
    "        \n",
    "        return dfs\n",
    "        \n",
    "    def transform(self, test_pathes: List[str]):\n",
    "        test_texts = self._get_texts_dicts(test_pathes)    \n",
    "        print('len(test_texts)={}\\n'.format(len(test_texts)))\n",
    "        \n",
    "        dfs = {}\n",
    "        for k in self.ks:\n",
    "            print('k={}'.format(k))\n",
    "            dfs[k] = self.create_kgramm_df(k=k, txts=test_texts)\n",
    "            print('\\n')\n",
    "            \n",
    "        return dfs\n",
    "\n",
    "    def _read_txt(self, path: str) -> Optional[str]:\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as fd:\n",
    "                return fd.read()\n",
    "        except Exception as e:\n",
    "            print('exception reading path {}: {}'.format(path, e))\n",
    "        return None\n",
    "    \n",
    "    def _preproc_txt(self, txt: str) -> Optional[str]:\n",
    "        txt = txt.lower()\n",
    "\n",
    "        buff = []\n",
    "        for c in txt:\n",
    "            buff.append(' ' if c not in self.alphabet else c)\n",
    "        txt = ''.join(buff)\n",
    "\n",
    "        txt = re.sub(\"\\s\\s+\" , \" \", txt)\n",
    "\n",
    "        words = txt.split(' ')\n",
    "        words = [w for w in words if w not in self.sw]\n",
    "        txt = ' '.join(words)\n",
    "        txt = ''.join(txt.split(' '))\n",
    "        return txt\n",
    "    \n",
    "    def _collect_kgramm_stat_text(self, txt: str, k: int) -> Dict[str, int]:\n",
    "        d = {}\n",
    "        tlen = len(txt)\n",
    "        for i in range(tlen-k):\n",
    "            kgr = txt[i:i+k]\n",
    "            if kgr not in d:\n",
    "                d[kgr] = 0\n",
    "            d[kgr] += 1\n",
    "\n",
    "        return d\n",
    "    \n",
    "    def _collect_kgramm_stat(self, k: int) -> Dict[str, int]:\n",
    "        stat = {}\n",
    "        for d in self.train_texts:\n",
    "            n = d['num']\n",
    "            p = d['path']\n",
    "            t = d['text']\n",
    "            print('processing text {}'.format(p))\n",
    "            \n",
    "            d = self._collect_kgramm_stat_text(t, k)\n",
    "            \n",
    "            for kgr,v in d.items():\n",
    "                if kgr not in stat:\n",
    "                    stat[kgr] = 0\n",
    "                stat[kgr] += v\n",
    "                \n",
    "        return stat\n",
    "    \n",
    "    def _get_kgramm_encoder(self) -> Dict[str, int]:\n",
    "        enc_dict = {}\n",
    "        for k in self.ks:\n",
    "            print('processing k={}'.format(k))\n",
    "            d = self._collect_kgramm_stat(k)\n",
    "            \n",
    "            enc_id_to_kgr = {1: '<OTHER>'}\n",
    "            enc_kgr_to_id = {'<OTHER>': 1}\n",
    "            kgr_prob = {'<OTHER>': 0}\n",
    "            \n",
    "            cnt_sum = sum(d.values())\n",
    "            \n",
    "            n = 2\n",
    "            min_prob = None\n",
    "            other_cnt = 0\n",
    "            for kgr, cnt in sorted(d.items(), key=lambda x: -x[1]):\n",
    "                if cnt<self.min_occurrence[k]:\n",
    "                    other_cnt += cnt\n",
    "                    continue\n",
    "                enc_id_to_kgr[n] = kgr\n",
    "                enc_kgr_to_id[kgr] = n\n",
    "                kgr_prob[kgr] = cnt/cnt_sum\n",
    "                if min_prob is None or min_prob>kgr_prob[kgr]:\n",
    "                    min_prob = kgr_prob[kgr]\n",
    "                n += 1\n",
    "                \n",
    "            kgr_prob['<OTHER>'] = other_cnt/cnt_sum\n",
    "            \n",
    "            enc_dict[k] = {'id_to_kgr':enc_id_to_kgr, 'kgr_to_id':enc_kgr_to_id, 'kgr_prob':kgr_prob}\n",
    "            print('len(k)={}/{} other_prob={:0.6f}, min_prob={:0.6f}\\n'.format(\n",
    "                    len(enc_id_to_kgr), self.alen**k, kgr_prob['<OTHER>'], min_prob\n",
    "                )\n",
    "            )\n",
    "                \n",
    "        return enc_dict\n",
    "\n",
    "    def create_kgramm_df(self, k: int, txts: Optional[List] = None) -> Optional[pd.DataFrame]:\n",
    "        data = []\n",
    "        encoder = self.kgramm_enc[k]['kgr_to_id']\n",
    "        allids = list(self.kgramm_enc[k]['kgr_to_id'].values())\n",
    "        len_allids = len(allids)\n",
    "            \n",
    "        for d in txts:\n",
    "            n = d['num']\n",
    "            p = d['path']\n",
    "            t = d['text']\n",
    "            print('processing text {}'.format(p))\n",
    "            \n",
    "            tlen = len(t)\n",
    "            for i in range(tlen-(k+self.left_cnt*k+self.right_cnt*k+k)):\n",
    "                lefts = []\n",
    "                for l in range(self.left_cnt):\n",
    "                    kgr = t[l*k+i: l*k+i+k]\n",
    "                    lefts.append(kgr)\n",
    "                #print(lefts)\n",
    "                lefts = [encoder.get(s,1) for s in lefts]\n",
    "                \n",
    "                l += 1\n",
    "                target_kgr = t[l*k+i: l*k+i+k]\n",
    "                #print(target_kgr)\n",
    "                target_kgr = encoder.get(target_kgr,1)\n",
    "                l += 1\n",
    "                \n",
    "                rights = []\n",
    "                for r in range(self.right_cnt):\n",
    "                    kgr = t[r*k+l*k+i: r*k+l*k+i+k]\n",
    "                    rights.append(kgr)\n",
    "                #print(rights)\n",
    "                rights = [encoder.get(s,1) for s in rights]\n",
    "                \n",
    "                if 'rnd' not in p:\n",
    "                    data.append([n, i] + lefts + [target_kgr] + rights + [1])\n",
    "                while True:\n",
    "                    rnd_kgr = allids[np.random.randint(len_allids)]\n",
    "                    if rnd_kgr != target_kgr:\n",
    "                        data.append([n, i] + lefts + [rnd_kgr] + rights + [0])\n",
    "                        break\n",
    "                #print(t[i:i+1000])\n",
    "                #1/0\n",
    "        \n",
    "        left_cols = ['l{}'.format(n) for n in range(self.left_cnt)]\n",
    "        right_cols = ['r{}'.format(n) for n in range(self.right_cnt)]\n",
    "        return pd.DataFrame(data=data, columns=['n', 'i']+left_cols+['tkgr']+right_cols+['target'])\n",
    "    \n",
    "    def _rnd_txt(self, tlen):\n",
    "        return ''.join(self.alphabet[i] for i in np.random.randint(self.alen, size=tlen))\n",
    "    \n",
    "    def _insert_rnd(self, txt, nins):\n",
    "        tlen = len(txt)\n",
    "        ntxt = str(txt)\n",
    "        for n in range(nins):\n",
    "            ins_idx = np.random.randint(tlen)\n",
    "            ntxt = ntxt[:ins_idx]+self.alphabet[np.random.randint(self.alen)]+ntxt[ins_idx:]\n",
    "        return ntxt[:tlen]\n",
    "    \n",
    "    def _insert_rnd_seq(self, txt, nins):\n",
    "        tlen = len(txt)\n",
    "        ntxt = str(txt)\n",
    "        ins_chars = []\n",
    "        for n in range(nins):\n",
    "            ins_chars.append(self.alphabet[np.random.randint(self.alen)])\n",
    "        \n",
    "        ins_txt = ''.join(ins_chars)\n",
    "        ins_idx = np.random.randint(tlen)\n",
    "        ntxt = ntxt[:ins_idx]+ins_txt+ntxt[ins_idx:]\n",
    "        return ntxt[:tlen]\n",
    "    \n",
    "    def _replace_rnd(self, txt, nins):\n",
    "        tlen = len(txt)\n",
    "        ntxt = str(txt)\n",
    "        for n in range(nins):\n",
    "            ins_idx = np.random.randint(tlen)\n",
    "            ntxt = ntxt[:ins_idx]+self.alphabet[np.random.randint(self.alen)]+ntxt[ins_idx+1:]\n",
    "        return ntxt[:tlen]\n",
    "    \n",
    "    def get_openclosed_text_df(self, test_pathes: List[str], clen: int, keep_prob: float) -> pd.DataFrame:\n",
    "        data = []\n",
    "        \n",
    "        test_texts = self._get_texts_dicts(test_pathes)\n",
    "        print('len(test_texts)={}'.format(len(test_texts)))\n",
    "        \n",
    "        for d in test_texts:\n",
    "            n = d['num']\n",
    "            p = d['path']\n",
    "            t = d['text']\n",
    "            print('processing text {}'.format(p))\n",
    "            maxlen = len(t)-clen\n",
    "            for i in range(maxlen):\n",
    "                if np.random.rand()>keep_prob:\n",
    "                    continue\n",
    "                \n",
    "                if i%12345 == 0:\n",
    "                    print('{}/{} done;'.format(i, maxlen))\n",
    "                \n",
    "                txt = t[i:i+clen]\n",
    "                if len(txt)!=clen:\n",
    "                    continue\n",
    "                \n",
    "                # open text\n",
    "                dtype = np.random.randint(low=0, high=3)\n",
    "                sr = np.random.randint(low=self.min_error_len, high=self.max_error_len)\n",
    "                if dtype==0:\n",
    "                    data.append([n, i, dtype, self._insert_rnd(txt, sr), sr, 1])\n",
    "                if dtype==1:\n",
    "                    data.append([n, i, dtype, self._insert_rnd_seq(txt, sr), sr, 1])\n",
    "                else:\n",
    "                    data.append([n, i, dtype, self._replace_rnd(txt, sr), sr, 1])\n",
    "                \n",
    "                # closed text\n",
    "                data.append([n, i, dtype, self._rnd_txt(clen), sr, 0])\n",
    "                \n",
    "        \n",
    "        return pd.DataFrame(data=data, columns=['n_txt', 'idx', 'bad_type', 'txt', 'num_reps', 'open'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otp = OpenTextProcessor(\n",
    "    min_occurrence={2: 80, 3:120},\n",
    "    left_cnt=6, right_cnt=3,\n",
    "    min_error_len=0, max_error_len=1,\n",
    "    sw={}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dfs = otp.fit(train_pathes)\n",
    "\n",
    "for k in train_dfs:\n",
    "    print('k={}'.format(k))\n",
    "    path_save = path_root+'train_k2v_{}.csv'.format(k)\n",
    "    train_dfs[k].to_csv(path_save, index=False)\n",
    "    print('saved to {}'.format(path_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dfs = otp.transform(test_pathes)\n",
    "\n",
    "for k in test_dfs:\n",
    "    print('k={}'.format(k))\n",
    "    path_save = path_root+'test_k2v_{}.csv'.format(k)\n",
    "    test_dfs[k].to_csv(path_save, index=False)\n",
    "    print('saved to {}'.format(path_save))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_root + 'encoder.json', 'w') as fd:\n",
    "    encjs = json.dumps(otp.kgramm_enc)\n",
    "    fd.write(encjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# open/cipher text nn data\n",
    "df_data = otp.get_openclosed_text_df(test_pathes, clen=44, keep_prob=0.4)\n",
    "path_train_data = path_root + 'open_closed_nosw.csv'\n",
    "df_data.to_csv(path_train_data, index=False)\n",
    "print('df_data.shape={}'.format(df_data.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data.sample(frac=0.1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
